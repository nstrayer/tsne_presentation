---
title: "Visualizing Data using t-SNE"
subtitle: "An article review"
author: "Nick Strayer"
date: "2016/12/12"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

# Organization

- Summary
- Motivation
- Stochastic Neighbor Embedding
- t-SNE
- Results
---
# Summary

- A visualization tool for high dimensional data
  - Makes maps that 'reveal structure at multiple scales'

- Stochastic method based upon older 'SNE' method

- Two main contributions
  1. Converting data matrix to pairwise similarities
  2. Mapping those similarities to a lower-dimensional space
---

# Revealing structure at multiple scales

What does this mean?

> Good for high dimensional data that lie on several different, but related, low-dimensional manifolds."

.center[
<img src = "https://skybluetrades.net/blog/posts/2011/10/30/machine-learning/test-swiss-roll.png" height = 350 />
]


---
# Motivation

- Data is ever expanding!

- No matter how much we may want, data will keep getting higher and higher dimension. 

- We have tackled this problem in two different ways.
  - Tools that can represent higher dimensions of data
  - Those that map high dimensional data to a lower dimensional space.

---
# Tools for representing more fields

- Chernoff faces, graph representations, etc.
- These visualize more dimensions but they still only have a finite amount of dimensions they can represnt. 
  - You're not going to visualize 5k protein expression profiles with a Chernoff face.
  
![Chernoff Face](chernoff.png)
---
# Mapping to a lower dimension

- Let's use some math to lower the dimensionality of our data. 

$$\underline{X} \to \underline{Y}; \underline{X} \in \mathrel{R_k}, \underline{Y} \in \mathrel{R_{2 \text{ or } 3}}$$

- Classic example is PCA. 
  - Transformation is linear and doesn't pay attention to manifolds or local structure at all. 
  - Just maximizes variance. 
  
> For high-dimensional data that lies on or near a low-dimensional, non-linear manifold it is usually more important to keep the low-dimensional representation of very similar datapoints close together, which is typically not possible with linear mapping.


---
# Non-linear mapping

- Sammon mappings, Maxmimum Variance Unfolding, Laplacien Eigenmaps, Isomap, etc.

- Have existed for a while but leave a bit to be desired. 

- Maxmimum Variance Unfolding can't seperate MNIST at all.

- All the papers introducing them show great results on simulated data, but real world results never panned out. 

---
# Stochastic neighbor embedding (SNE)

- Precursor to t-sne and very similar

- Converts high-dimensional euclidean distances between two observations to conditional probabilties.

- Purely guassian (more on this shortly.)

---
# SNE algorithm

1. Calculate pairwise distances of all points in your dataset. 
2. Randomly initialize map positions $y_i$ for all $i \in X$.
2. Plop a multi-dimensional gaussian distribution on top of each datapoint in your high-dimensional space
3. For each pair of $x_i$ to $x_j$ calculate $p_{i|j}$.
  - Using $p_{i|j} = \frac{\exp{(-||x_i-x_j||^2/2\sigma_i^2)}}{\sum_{k \ne i} \exp{(-||x_i - x_k||^2/2\sigma_i^2)}}$
4. Calculate the conditional probabilities for the mappings.
  - Using $q_{i|j} = \frac{\exp{(-||y_i-y_j||^2)}}{\sum_{k \ne i} \exp{(-||y_i - y_k||^2)}}$
6. Calculate the KLD between your $p$ and $q$s.
  - Using $C = \sum_i\sum_jp_{i|j} \frac{p_{i|j}}{q_{i|j}}$
7. Minimize this by gradient descent! 
---


# T-distributed stochastic neighbor embedding

---
# Results

---
# Extesions?